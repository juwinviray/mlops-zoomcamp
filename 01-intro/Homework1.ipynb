{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e364c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1b9cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/codespace/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.4 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /home/codespace/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 67.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /home/codespace/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 2.2.0\n",
      "    Uninstalling threadpoolctl-2.2.0:\n",
      "      Successfully uninstalled threadpoolctl-2.2.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# root_mean_square_error is in sklearn 1.4.+\n",
    "# !pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a84044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 39.9 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /home/codespace/anaconda3/lib/python3.9/site-packages (from pyarrow) (1.21.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-17.0.0\n"
     ]
    }
   ],
   "source": [
    "# this is needed to read parquet files... only need to do it once\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be175c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1: Read the data for January. How many columns are there?\n",
    "df = pd.read_parquet('./data/yellow_tripdata_2023-01.parquet')\n",
    "# df.head()\n",
    "# Q1: ANSWER\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbce1186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.594351241920904"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2: What's the standard deviation of the trips duration in January?\n",
    "df = pd.read_parquet('./data/yellow_tripdata_2023-01.parquet')\n",
    "\n",
    "df['duration'] =  df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "df['duration'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c24360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Next, we need to check the distribution of the duration variable. \n",
    "# There are some outliers. Let's remove them and keep only the records \n",
    "# where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "# What fraction of the records left after you dropped the outliers?\n",
    "before = len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fd5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct records left after outliers: 0.9812202822125979\n"
     ]
    }
   ],
   "source": [
    "df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "after = len(df)\n",
    "\n",
    "print(f\"pct records left after outliers: {after/before}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2071e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Let's apply one-hot encoding to the pickup and dropoff location IDs. \n",
    "# We'll use only these two features for our model.\n",
    "\n",
    "# Turn the dataframe into a list of dictionaries \n",
    "# (remember to re-cast the ids to strings - otherwise it will label encode them)\n",
    "# Fit a dictionary vectorizer\n",
    "# Get a feature matrix from it\n",
    "# What's the dimensionality of this matrix (number of columns)?\n",
    "\n",
    "# define features for one-hot encoding, cast to strings\n",
    "# PULocation & DOLocation are ints it will try and label\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']\n",
    "df[categorical] = df[categorical].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca86dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need dicts\n",
    "train_dicts = df[categorical + numerical].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e4eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary vectorizer\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(train_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8229af44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3009173x516 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9027519 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature matrix\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e68651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSWER: 516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a315b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Now let's use the feature matrix from the previous step to train a model.\n",
    "\n",
    "# Train a plain linear regression model with default parameters, where duration is the response variable\n",
    "# Calculate the RMSE of the model on the training data\n",
    "# What's the RMSE on train?\n",
    "\n",
    "# get the target values\n",
    "target = 'duration'\n",
    "y_train = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2effaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.649143388169879"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use linear regression to train x to Y\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_train)\n",
    "\n",
    "root_mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: Now let's apply this model to the validation dataset (February 2023).\n",
    "\n",
    "#What's the RMSE on validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8cc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "        df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "    elif filename.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdfac1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the dataframes\n",
    "df_train = read_dataframe('./data/yellow_tripdata_2023-01.parquet')\n",
    "df_val = read_dataframe('./data/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07f78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up one-encoding\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685fdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()\n",
    "\n",
    "# training inputs\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# predictive inputs\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffda13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the Y targets for training data and predictive data\n",
    "\n",
    "target = 'duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
